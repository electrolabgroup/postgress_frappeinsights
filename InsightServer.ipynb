{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce628f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import pandas as pd\n",
    "import requests\n",
    "import logging\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)  # Adjust level as needed\n",
    "\n",
    "# Function to check if table exists\n",
    "def table_exists(cursor, table_name):\n",
    "    cursor.execute(\"\"\"\n",
    "    SELECT EXISTS (\n",
    "       SELECT 1\n",
    "       FROM   information_schema.tables \n",
    "       WHERE  table_schema = 'public'\n",
    "       AND    table_name = %s\n",
    "    );\n",
    "    \"\"\", (table_name,))\n",
    "    return cursor.fetchone()[0]\n",
    "\n",
    "# Function to handle data retrieval and insertion for Opportunity\n",
    "def fetch_and_insert_data_opportunity(url, params, headers, insert_query, cursor):\n",
    "    limit_start = 0\n",
    "    limit_page_length = 1000\n",
    "    all_data = []\n",
    "    while True:\n",
    "        params['limit_start'] = limit_start\n",
    "        try:\n",
    "            response = requests.get(url, params=params, headers=headers, timeout=10)\n",
    "            response.raise_for_status()\n",
    "            data = response.json()\n",
    "            if 'data' in data:\n",
    "                current_page_data = data['data']\n",
    "                all_data.extend(current_page_data)\n",
    "                if len(current_page_data) < limit_page_length:\n",
    "                    break \n",
    "                else:\n",
    "                    limit_start += limit_page_length  \n",
    "            else:\n",
    "                break  \n",
    "        except requests.exceptions.RequestException as e:\n",
    "            logging.error(f\"Error fetching data: {e}\")\n",
    "            break\n",
    "\n",
    "    # Filter out records without name\n",
    "    all_data_filtered = [record for record in all_data if 'name' in record and record['name'] is not None]\n",
    "\n",
    "    # Normalize JSON data into a DataFrame\n",
    "    opportunity = pd.json_normalize(all_data_filtered)\n",
    "\n",
    "    # Insert data into PostgreSQL\n",
    "    try:\n",
    "        for row in opportunity.itertuples(index=False):\n",
    "            cursor.execute(insert_query, (\n",
    "                row.name,\n",
    "                row.deal_pipeline,\n",
    "                row.export_opportunity_amount,\n",
    "                row.transaction_date,\n",
    "                row.status,\n",
    "                row.opportunity_amount,\n",
    "                row.modified  # Last Updated On\n",
    "            ))\n",
    "        connection.commit()\n",
    "        logging.info(\"Data insertion successful.\")\n",
    "    except psycopg2.Error as e:\n",
    "        logging.error(f\"Error inserting data into PostgreSQL: {e}\")\n",
    "        connection.rollback()  # Rollback changes in case of error\n",
    "\n",
    "    return opportunity  # Return the DataFrame after insertion\n",
    "\n",
    "# Define connection details for PostgreSQL\n",
    "db_config = {\n",
    "    'host': '192.168.2.11',\n",
    "    'user': 'postgres',\n",
    "    'password': 'admin@123',\n",
    "    'dbname': 'postgres'\n",
    "}\n",
    "\n",
    "# Connect to PostgreSQL\n",
    "try:\n",
    "    connection = psycopg2.connect(**db_config)\n",
    "    cursor = connection.cursor()\n",
    "\n",
    "    # Define table name for Opportunity\n",
    "    table_name_opportunity = 'Opportunity'\n",
    "\n",
    "    # Check if Opportunity table exists, create if it doesn't\n",
    "    if not table_exists(cursor, table_name_opportunity):\n",
    "        create_table_query = \"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS Opportunity (\n",
    "            name VARCHAR(255) PRIMARY KEY,\n",
    "            deal_pipeline VARCHAR(255),\n",
    "            export_opportunity_amount DECIMAL(15, 2),\n",
    "            transaction_date DATE,\n",
    "            status VARCHAR(50),\n",
    "            opportunity_amount DECIMAL(15, 2),\n",
    "            last_updated_on TIMESTAMP  -- New field for Last Updated On\n",
    "        )\n",
    "        \"\"\"\n",
    "        cursor.execute(create_table_query)\n",
    "        logging.info(\"Opportunity table created.\")\n",
    "\n",
    "    # Define API endpoint and parameters for Opportunity\n",
    "    base_url = 'https://erpv14.electrolabgroup.com/'\n",
    "    endpoint = 'api/resource/Opportunity'\n",
    "    url = base_url + endpoint\n",
    "    params = {\n",
    "        'fields': '[\"name\",\"deal_pipeline\",\"export_opportunity_amount\",\"transaction_date\",\"status\",\"opportunity_amount\",\"modified\"]',\n",
    "        'limit_page_length': 1000\n",
    "    }\n",
    "    headers = {\n",
    "        'Authorization': 'token 3ee8d03949516d0:6baa361266cf807'\n",
    "    }\n",
    "\n",
    "    # Define INSERT query with conflict resolution for Opportunity\n",
    "    insert_query = \"\"\"\n",
    "    INSERT INTO Opportunity (name, deal_pipeline, export_opportunity_amount, transaction_date, status, opportunity_amount, last_updated_on)\n",
    "    VALUES (%s, %s, %s, %s, %s, %s, %s)\n",
    "    ON CONFLICT (name) DO UPDATE\n",
    "    SET\n",
    "        deal_pipeline = EXCLUDED.deal_pipeline,\n",
    "        export_opportunity_amount = EXCLUDED.export_opportunity_amount,\n",
    "        transaction_date = EXCLUDED.transaction_date,\n",
    "        status = EXCLUDED.status,\n",
    "        opportunity_amount = EXCLUDED.opportunity_amount,\n",
    "        last_updated_on = EXCLUDED.last_updated_on  -- Update for Last Updated On\n",
    "    \"\"\"\n",
    "\n",
    "    # Fetch data and insert into PostgreSQL for Opportunity, and capture DataFrame\n",
    "    opportunity_df = fetch_and_insert_data_opportunity(url, params, headers, insert_query, cursor)\n",
    "\n",
    "    # Now you can use opportunity_df for any further processing or analysis\n",
    "\n",
    "except psycopg2.Error as e:\n",
    "    logging.error(f\"Error connecting to PostgreSQL: {e}\")\n",
    "\n",
    "finally:\n",
    "    # Close cursor and connection\n",
    "    if 'connection' in locals():\n",
    "        cursor.close()\n",
    "        connection.close()\n",
    "        logging.info(\"PostgreSQL connection closed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc0f19d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a0bb6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import pandas as pd\n",
    "import requests\n",
    "import logging\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)  # Adjust level as needed\n",
    "\n",
    "# Function to check if table exists\n",
    "def table_exists(cursor, table_name):\n",
    "    cursor.execute(\"\"\"\n",
    "    SELECT EXISTS (\n",
    "       SELECT 1\n",
    "       FROM   information_schema.tables \n",
    "       WHERE  table_schema = 'public'\n",
    "       AND    table_name = %s\n",
    "    );\n",
    "    \"\"\", (table_name,))\n",
    "    return cursor.fetchone()[0]\n",
    "\n",
    "# Function to handle data retrieval and insertion for Sales Order\n",
    "def fetch_and_insert_data_sales_order(url, params, headers, insert_query, cursor):\n",
    "    limit_start = 0\n",
    "    limit_page_length = 1000\n",
    "    all_data = []\n",
    "    while True:\n",
    "        params['limit_start'] = limit_start\n",
    "        try:\n",
    "            response = requests.get(url, params=params, headers=headers, timeout=10)\n",
    "            response.raise_for_status()\n",
    "            data = response.json()\n",
    "            if 'data' in data:\n",
    "                current_page_data = data['data']\n",
    "                all_data.extend(current_page_data)\n",
    "                if len(current_page_data) < limit_page_length:\n",
    "                    break \n",
    "                else:\n",
    "                    limit_start += limit_page_length  \n",
    "            else:\n",
    "                break  \n",
    "        except requests.exceptions.RequestException as e:\n",
    "            logging.error(f\"Error fetching data: {e}\")\n",
    "            break\n",
    "\n",
    "    # Filter out records without name\n",
    "    all_data_filtered = [record for record in all_data if 'name' in record and record['name'] is not None]\n",
    "\n",
    "    # Normalize JSON data into a DataFrame\n",
    "    sales_order = pd.json_normalize(all_data_filtered)\n",
    "\n",
    "    # Insert data into PostgreSQL\n",
    "    try:\n",
    "        for row in sales_order.itertuples(index=False):\n",
    "            cursor.execute(insert_query, (\n",
    "                row.name,\n",
    "                row.transaction_date,\n",
    "                row.net_total,\n",
    "                row.naming_series\n",
    "            ))\n",
    "        connection.commit()\n",
    "        logging.info(\"Data insertion successful.\")\n",
    "    except psycopg2.Error as e:\n",
    "        logging.error(f\"Error inserting data into PostgreSQL: {e}\")\n",
    "        connection.rollback()  # Rollback changes in case of error\n",
    "    \n",
    "    # Return the DataFrame\n",
    "    return sales_order\n",
    "\n",
    "# Define connection details for PostgreSQL\n",
    "db_config = {\n",
    "    'host': '192.168.2.11',\n",
    "    'user': 'postgres',\n",
    "    'password': 'admin@123',\n",
    "    'dbname': 'postgres'\n",
    "}\n",
    "\n",
    "# Connect to PostgreSQL\n",
    "try:\n",
    "    connection = psycopg2.connect(**db_config)\n",
    "    cursor = connection.cursor()\n",
    "\n",
    "    # Define table name for Sales Order\n",
    "    table_name_sales_order = 'SalesOrder'\n",
    "\n",
    "    # Check if Sales Order table exists, create if it doesn't\n",
    "    if not table_exists(cursor, table_name_sales_order):\n",
    "        create_table_query = \"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS SalesOrder (\n",
    "            name VARCHAR(255) PRIMARY KEY,\n",
    "            transaction_date DATE,\n",
    "            net_total DECIMAL(15, 2),\n",
    "            naming_series VARCHAR(255)\n",
    "        )\n",
    "        \"\"\"\n",
    "        cursor.execute(create_table_query)\n",
    "        logging.info(\"SalesOrder table created.\")\n",
    "\n",
    "    # Define API endpoint and parameters for Sales Order\n",
    "    base_url = 'https://erpv14.electrolabgroup.com/'\n",
    "    endpoint = 'api/resource/Sales Order'\n",
    "    url = base_url + endpoint\n",
    "    params = {\n",
    "        'fields': '[\"name\",\"transaction_date\",\"net_total\",\"naming_series\"]',\n",
    "        'limit_page_length': 1000\n",
    "    }\n",
    "    headers = {\n",
    "        'Authorization': 'token 3ee8d03949516d0:6baa361266cf807'  # Adjust authorization token\n",
    "    }\n",
    "\n",
    "    # Define INSERT query with conflict resolution for Sales Order\n",
    "    insert_query = \"\"\"\n",
    "    INSERT INTO SalesOrder (name, transaction_date, net_total, naming_series)\n",
    "    VALUES (%s, %s, %s, %s)\n",
    "    ON CONFLICT (name) DO UPDATE\n",
    "    SET\n",
    "        transaction_date = EXCLUDED.transaction_date,\n",
    "        net_total = EXCLUDED.net_total,\n",
    "        naming_series = EXCLUDED.naming_series\n",
    "    \"\"\"\n",
    "\n",
    "    # Fetch data and insert into PostgreSQL for Sales Order\n",
    "    sales_order_df = fetch_and_insert_data_sales_order(url, params, headers, insert_query, cursor)\n",
    "    logging.info(f\"Data fetched and inserted into PostgreSQL. DataFrame shape: {sales_order_df.shape}\")\n",
    "\n",
    "except psycopg2.Error as e:\n",
    "    logging.error(f\"Error connecting to PostgreSQL: {e}\")\n",
    "\n",
    "finally:\n",
    "    # Close cursor and connection\n",
    "    if 'connection' in locals():\n",
    "        cursor.close()\n",
    "        connection.close()\n",
    "        logging.info(\"PostgreSQL connection closed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "861c328f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'transaction_date' and 'modified' to datetime\n",
    "opportunity_df['transaction_date'] = pd.to_datetime(opportunity_df['transaction_date'])\n",
    "opportunity_df['modified'] = pd.to_datetime(opportunity_df['modified'])\n",
    "\n",
    "# Get current date\n",
    "current_date = datetime.now()\n",
    "\n",
    "# Filter conditions\n",
    "transaction_date_condition = opportunity_df['transaction_date'].dt.month != current_date.month\n",
    "status_condition = opportunity_df['status'].isin(['Closed', 'Converted', 'Order Won', 'Lost', 'Order Lost'])\n",
    "deal_pipeline_condition = opportunity_df['deal_pipeline'].str.contains('export', case=False)\n",
    "modified_condition = (opportunity_df['modified'].dt.year == current_date.year) & (opportunity_df['modified'].dt.month == current_date.month)\n",
    "\n",
    "# Apply filters\n",
    "opportunity_df_filtered = opportunity_df[\n",
    "    transaction_date_condition &\n",
    "    status_condition &\n",
    "    deal_pipeline_condition &\n",
    "    modified_condition\n",
    "]\n",
    "opportunity_df_filtered.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0adb4118",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'transaction_date' and 'modified' to datetime\n",
    "opportunity_df_filtered['transaction_date'] = pd.to_datetime(opportunity_df_filtered['transaction_date'])\n",
    "opportunity_df_filtered['modified'] = pd.to_datetime(opportunity_df_filtered['modified'])\n",
    "\n",
    "# Extract month and year from 'transaction_date'\n",
    "opportunity_df_filtered['transaction_month'] = opportunity_df_filtered['transaction_date'].dt.to_period('M')\n",
    "\n",
    "# Group by 'deal_pipeline' and 'transaction_month', and sum 'export_opportunity_amount'\n",
    "opportunity = opportunity_df_filtered.groupby(['deal_pipeline', 'transaction_month'])['export_opportunity_amount'].sum().reset_index()\n",
    "opportunity.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f7e07d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace the values in 'deal_pipeline' column with 'Export'\n",
    "opportunity['deal_pipeline'] = 'Export'\n",
    "\n",
    "# Group by 'transaction_month' and 'deal_pipeline', and sum 'export_opportunity_amount'\n",
    "opp_export_df = opportunity.groupby(['deal_pipeline'])['export_opportunity_amount'].sum().reset_index()\n",
    "opp_export_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d2b754",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the current month in the format 'YYYY-MM'\n",
    "current_month = datetime.now().strftime('%Y-%m')\n",
    "\n",
    "# Add the current month as a new column\n",
    "opp_export_df['month'] = current_month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca70c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "opp_export_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e9149e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the current date\n",
    "current_date = datetime.now()\n",
    "\n",
    "# Filter out current month\n",
    "carry_export = opportunity_df[opportunity_df['transaction_date'].dt.month != current_date.month]\n",
    "\n",
    "carry_export.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b36508",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exclude specific statuses\n",
    "excluded_statuses = ['Closed', 'Converted', 'Order Won', 'Lost', 'Order Lost']\n",
    "carry_export = carry_export[~carry_export['status'].isin(excluded_statuses)]\n",
    "\n",
    "carry_export.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed62f32b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter deal_pipeline to include 'export' (case insensitive)\n",
    "carry_export = carry_export.dropna(subset=['deal_pipeline'])\n",
    "deal_pipeline_condition = carry_export['deal_pipeline'].str.contains('export', case=False)\n",
    "carry_export = carry_export[deal_pipeline_condition]\n",
    "carry_export.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "322f466c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract month and year from 'transaction_date'\n",
    "carry_export['transaction_month'] = carry_export['transaction_date'].dt.to_period('M')\n",
    "\n",
    "# Group by 'deal_pipeline' and 'transaction_month', and sum 'export_opportunity_amount'\n",
    "carry_export = carry_export.groupby(['deal_pipeline', 'transaction_month'])['export_opportunity_amount'].sum().reset_index()\n",
    "carry_export.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "988bf960",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace the values in 'deal_pipeline' column with 'Export'\n",
    "carry_export['deal_pipeline'] = 'Export'\n",
    "\n",
    "# Group by 'transaction_month' and 'deal_pipeline', and sum 'export_opportunity_amount'\n",
    "carry_export = carry_export.groupby(['deal_pipeline'])['export_opportunity_amount'].sum().reset_index()\n",
    "carry_export.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3be9737",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_month = datetime.now().strftime('%Y-%m')\n",
    "\n",
    "# Add the current month as a new column\n",
    "carry_export['month'] = current_month\n",
    "carry_export.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f55e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'transaction_date' and 'modified' to datetime\n",
    "opportunity_df['transaction_date'] = pd.to_datetime(opportunity_df['transaction_date'])\n",
    "opportunity_df['modified'] = pd.to_datetime(opportunity_df['modified'])\n",
    "\n",
    "# Get current date\n",
    "current_date = datetime.now()\n",
    "\n",
    "# Filter conditions\n",
    "transaction_date_condition = opportunity_df['transaction_date'].dt.month != current_date.month\n",
    "status_condition = opportunity_df['status'].isin(['Closed', 'Converted', 'Order Won', 'Lost', 'Order Lost'])\n",
    "\n",
    "modified_condition = (opportunity_df['modified'].dt.year == current_date.year) & (opportunity_df['modified'].dt.month == current_date.month)\n",
    "\n",
    "# Apply filters\n",
    "opportunity_df_filtered = opportunity_df[\n",
    "    transaction_date_condition &\n",
    "    status_condition &\n",
    "    modified_condition\n",
    "]\n",
    "opportunity_df_filtered.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c294861",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'transaction_date' and 'modified' to datetime\n",
    "opportunity_df_filtered['transaction_date'] = pd.to_datetime(opportunity_df_filtered['transaction_date'])\n",
    "opportunity_df_filtered['modified'] = pd.to_datetime(opportunity_df_filtered['modified'])\n",
    "\n",
    "# Extract month and year from 'transaction_date'\n",
    "opportunity_df_filtered['transaction_month'] = opportunity_df_filtered['transaction_date'].dt.to_period('M')\n",
    "\n",
    "# Group by 'deal_pipeline' and 'transaction_month', and sum 'export_opportunity_amount'\n",
    "opportunity = opportunity_df_filtered.groupby(['deal_pipeline', 'transaction_month'])['opportunity_amount'].sum().reset_index()\n",
    "opportunity.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa3dc5b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Group by 'transaction_month' and 'deal_pipeline', and sum 'export_opportunity_amount'\n",
    "opp_dom_df = opportunity.groupby(['deal_pipeline'])['opportunity_amount'].sum().reset_index()\n",
    "opp_dom_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e8f327c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the sum of opportunity amounts for 'Export Machine' and 'Export Spares'\n",
    "export_sum = opp_dom_df.loc[opp_dom_df['deal_pipeline'].isin(['Export Machine', 'Export Spares']), 'opportunity_amount'].sum()\n",
    "\n",
    "# Replace 'Export Machine' and 'Export Spares' with 'Export'\n",
    "opp_dom_df.loc[opp_dom_df['deal_pipeline'].isin(['Export Machine', 'Export Spares']), 'deal_pipeline'] = 'Export'\n",
    "\n",
    "# Group by 'deal_pipeline' and sum the opportunity amounts\n",
    "opp_dom_df = opp_dom_df.groupby('deal_pipeline', as_index=False).sum()\n",
    "opp_dom_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370b5545",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the current date\n",
    "current_date = datetime.now()\n",
    "\n",
    "# Filter out current month\n",
    "carry_domestic = opportunity_df[opportunity_df['transaction_date'].dt.month != current_date.month]\n",
    "\n",
    "carry_domestic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa85dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exclude specific statuses\n",
    "excluded_statuses = ['Closed', 'Converted', 'Order Won', 'Lost', 'Order Lost']\n",
    "carry_domestic = carry_domestic[~carry_domestic['status'].isin(excluded_statuses)]\n",
    "\n",
    "carry_domestic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc2d0f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter deal_pipeline to include 'export' (case insensitive)\n",
    "carry_domestic = carry_domestic.dropna(subset=['deal_pipeline'])\n",
    "carry_domestic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4048b852",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by 'transaction_month' and 'deal_pipeline', and sum 'export_opportunity_amount'\n",
    "carry_domestic = carry_domestic.groupby(['deal_pipeline'])['opportunity_amount'].sum().reset_index()\n",
    "carry_domestic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0c9b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Calculate the sum of opportunity amounts for 'Export Machine' and 'Export Spares'\n",
    "export_sum = carry_domestic.loc[carry_domestic['deal_pipeline'].isin(['Export Machine', 'Export Spares']), 'opportunity_amount'].sum()\n",
    "\n",
    "# Replace 'Export Machine' and 'Export Spares' with 'Export'\n",
    "carry_domestic.loc[carry_domestic['deal_pipeline'].isin(['Export Machine', 'Export Spares']), 'deal_pipeline'] = 'Export'\n",
    "\n",
    "# Group by 'deal_pipeline' and sum the opportunity amounts\n",
    "carry_domestic = carry_domestic.groupby('deal_pipeline', as_index=False).sum()\n",
    "carry_domestic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c35233",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "current_month = datetime.now().strftime('%Y-%m')\n",
    "\n",
    "# Add the current month as a new column\n",
    "carry_domestic['month'] = current_month\n",
    "carry_domestic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec3ae1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "current_month = datetime.now().strftime('%Y-%m')\n",
    "\n",
    "# Add the current month as a new column\n",
    "opp_dom_df['month'] = current_month\n",
    "opp_dom_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec715f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "carry_export.rename(columns = {'export_opportunity_amount':'opportunity_amount'}, inplace = True)\n",
    "carry_export.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ace26e",
   "metadata": {},
   "outputs": [],
   "source": [
    "opp_export_df.rename(columns = {'export_opportunity_amount':'opportunity_amount'}, inplace = True)\n",
    "opp_export_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cdad5a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate all dataframes\n",
    "all_data = pd.concat([carry_domestic, opp_dom_df, carry_export, opp_export_df])\n",
    "\n",
    "# Group by 'deal_pipeline' and sum 'opportunity_amount'\n",
    "opp_final = all_data.groupby('deal_pipeline', as_index=False)['opportunity_amount'].sum()\n",
    "opp_final.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7e7205",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of deal pipelines to be grouped under 'Machine'\n",
    "machine_pipelines = ['Machine', 'Peristaltic Pump', 'Star Series Pump', 'Formulation R & D',\n",
    "                     'Trial', 'Biowise', 'Bioreactor', 'Aquaflux', 'Product Specialist']\n",
    "\n",
    "# List of deal pipelines to be grouped under 'GastroSimPlus'\n",
    "gastrosimplus_pipelines = ['Gastro', 'SimPlus']\n",
    "\n",
    "# Create a new column 'group' to classify deal pipelines\n",
    "def classify_pipeline(pipeline):\n",
    "    if pipeline in machine_pipelines:\n",
    "        return 'Machine'\n",
    "    elif pipeline in gastrosimplus_pipelines:\n",
    "        return 'Gastro + Simplus'\n",
    "    else:\n",
    "        return pipeline\n",
    "\n",
    "opp_final['group'] = opp_final['deal_pipeline'].apply(classify_pipeline)\n",
    "\n",
    "# Group by the new column 'group' and sum the 'opportunity_amount'\n",
    "grouped_opp_final = opp_final.groupby('group', as_index=False)['opportunity_amount'].sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a57f2692",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter to keep only the specified groups\n",
    "desired_groups = ['Machine', 'Spares', 'Service', 'Export', 'Gastro + Simplus']\n",
    "filtered_grouped_opp_final = grouped_opp_final[grouped_opp_final['group'].isin(desired_groups)]\n",
    "filtered_grouped_opp_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c32dd39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename 'Service' to 'Assurance'\n",
    "filtered_grouped_opp_final['group'] = filtered_grouped_opp_final['group'].replace('Service', 'Assurance')\n",
    "\n",
    "filtered_grouped_opp_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f1f60cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_month = datetime.now().strftime('%Y-%m')\n",
    "\n",
    "# Add the current month as a new column\n",
    "filtered_grouped_opp_final['year_month'] = current_month\n",
    "filtered_grouped_opp_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "303833e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping of naming_series to new values\n",
    "naming_series_mapping = {\n",
    "    '2324SODM.####': 'Machine',\n",
    "    '2324SOEXP.####': 'Export',\n",
    "    '2324SODS.####': 'Spares',\n",
    "    '2324SOSA.####': 'Assurance',\n",
    "    '2324SOSP.####': 'Gastro + Simplus',\n",
    "    '2425SODM.####': 'Machine',\n",
    "    '2425SOEXP.####': 'Export',\n",
    "    '2425SODS.####': 'Spares',\n",
    "    '2425SOSA.####': 'Assurance',\n",
    "    '2425SOSP.####': 'Gastro + Simplus'\n",
    "}\n",
    "\n",
    "# Filter the dataframe to keep only the rows with the specified naming_series\n",
    "filtered_df = sales_order_df[sales_order_df['naming_series'].isin(naming_series_mapping.keys())]\n",
    "\n",
    "# Replace the naming_series values according to the mapping\n",
    "filtered_df['naming_series'] = filtered_df['naming_series'].replace(naming_series_mapping)\n",
    "\n",
    "# Replace the name values accordingly (example logic, you can adjust based on your specific rules)\n",
    "filtered_df['name'] = filtered_df['name'].apply(lambda x: x.replace('SODM', 'Machine')\n",
    "                                                            .replace('SOEXP', 'Export')\n",
    "                                                            .replace('SODS', 'Spares')\n",
    "                                                            .replace('SOSA', 'Assurance')\n",
    "                                                            .replace('SOSP', 'Gastro + Simplus'))\n",
    "filtered_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d8834e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'transaction_date' to datetime\n",
    "filtered_df['transaction_date'] = pd.to_datetime(filtered_df['transaction_date'])\n",
    "\n",
    "# Extract year and month\n",
    "filtered_df['year_month'] = filtered_df['transaction_date'].dt.to_period('M')\n",
    "\n",
    "# Group by 'year_month' and 'naming_series' and sum 'net_total'\n",
    "so_df = filtered_df.groupby(['year_month', 'naming_series'])['net_total'].sum().reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be34edcc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e303c140",
   "metadata": {},
   "outputs": [],
   "source": [
    "so_df.rename(columns = {'naming_series':'group'}, inplace = True)\n",
    "\n",
    "\n",
    "so_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "537b8a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'year_month' in filtered_grouped_opp_final from object to period[M]\n",
    "filtered_grouped_opp_final['year_month'] = pd.to_datetime(filtered_grouped_opp_final['year_month']).dt.to_period('M')\n",
    "\n",
    "print(so_df['year_month'].dtype)\n",
    "print(filtered_grouped_opp_final['year_month'].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a94ed834",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Perform the inner merge\n",
    "merged_df = pd.merge(so_df, filtered_grouped_opp_final, on=['year_month', 'group'], how='inner')\n",
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e4ab238",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the conversion ratio\n",
    "merged_df['conversion_ratio'] = merged_df['net_total'] / merged_df['opportunity_amount']\n",
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67fa96d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the ratio to a percentage\n",
    "merged_df['conversion_rate'] = np.round(merged_df['conversion_ratio'] * 100, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6670f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'year_month' to string before converting to datetime\n",
    "merged_df['year_month'] = merged_df['year_month'].astype(str)\n",
    "\n",
    "# Get the current date and time\n",
    "current_datetime = datetime.now()\n",
    "\n",
    "# Convert 'year_month' to datetime with current date and time\n",
    "merged_df['year_month'] = pd.to_datetime(merged_df['year_month'] + '-' + current_datetime.strftime('%d %H:%M:%S'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c360f0b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "170d2fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import logging\n",
    "from datetime import datetime\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)  # Adjust level as needed\n",
    "\n",
    "def prepare_dataframe(df):\n",
    "    # Check if 'year_month' column exists\n",
    "    if 'year_month' not in df.columns:\n",
    "        logging.error(\"DataFrame does not contain 'year_month' column.\")\n",
    "        raise KeyError(\"'year_month' column is missing from DataFrame.\")\n",
    "    \n",
    "    # Convert 'year_month' to datetime if it's not already in datetime format\n",
    "    if not pd.api.types.is_datetime64_any_dtype(df['year_month']):\n",
    "        df['year_month'] = pd.to_datetime(df['year_month'])\n",
    "    \n",
    "    # Normalize 'year_month' to the first day of the month\n",
    "    df['year_month'] = df['year_month'].dt.to_period('M').dt.to_timestamp()\n",
    "    \n",
    "    # Add 'last_update' column with current date\n",
    "    df['last_update'] = datetime.now().date()\n",
    "    \n",
    "    # Add 'Period' column in 'Month-YYYY' format\n",
    "    df['Period'] = df['year_month'].dt.strftime('%B-%Y')\n",
    "    \n",
    "    # Add 'year' column\n",
    "    df['year'] = df['year_month'].dt.year\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Define connection details for PostgreSQL\n",
    "db_config = {\n",
    "    'host': '192.168.2.11',\n",
    "    'user': 'postgres',\n",
    "    'password': 'admin@123',\n",
    "    'dbname': 'postgres'\n",
    "}\n",
    "\n",
    "# Function to drop table if it exists\n",
    "def drop_table(cursor, table_name):\n",
    "    cursor.execute(f\"DROP TABLE IF EXISTS {table_name}\")\n",
    "    logging.info(f\"{table_name} table dropped.\")\n",
    "\n",
    "# Function to create table\n",
    "def create_table(cursor):\n",
    "    create_table_query = \"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS SalesMetrics (\n",
    "        year_month DATE,\n",
    "        \"group\" VARCHAR(255),\n",
    "        net_total DECIMAL(15, 2),\n",
    "        opportunity_amount DECIMAL(15, 2),\n",
    "        conversion_ratio DECIMAL(10, 6),\n",
    "        conversion_rate DECIMAL(5, 2),\n",
    "        last_update DATE,\n",
    "        \"Period\" VARCHAR(12),\n",
    "        year INT,\n",
    "        PRIMARY KEY (year_month, \"group\")\n",
    "    )\n",
    "    \"\"\"\n",
    "    cursor.execute(create_table_query)\n",
    "    logging.info(\"SalesMetrics table created.\")\n",
    "\n",
    "# Function to check if current year_month exists\n",
    "def current_month_data_exists(cursor, current_year_month):\n",
    "    cursor.execute(\"\"\"\n",
    "    SELECT EXISTS (\n",
    "        SELECT 1\n",
    "        FROM SalesMetrics\n",
    "        WHERE year_month = %s\n",
    "    );\n",
    "    \"\"\", (current_year_month,))\n",
    "    return cursor.fetchone()[0]\n",
    "\n",
    "# Function to delete data for the current month\n",
    "def delete_current_month_data(cursor, current_year_month):\n",
    "    cursor.execute(\"\"\"\n",
    "    DELETE FROM SalesMetrics\n",
    "    WHERE year_month = %s\n",
    "    \"\"\", (current_year_month,))\n",
    "    connection.commit()\n",
    "    logging.info(\"Existing data for the current month deleted.\")\n",
    "\n",
    "# Function to insert or update data\n",
    "def insert_or_update_data(df, cursor):\n",
    "    df = prepare_dataframe(df)  # Ensure data types are compatible\n",
    "    \n",
    "    insert_query = \"\"\"\n",
    "    INSERT INTO SalesMetrics (year_month, \"group\", net_total, opportunity_amount, conversion_ratio, conversion_rate, last_update, \"Period\", year)\n",
    "    VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s)\n",
    "    ON CONFLICT (year_month, \"group\") DO UPDATE\n",
    "    SET\n",
    "        net_total = EXCLUDED.net_total,\n",
    "        opportunity_amount = EXCLUDED.opportunity_amount,\n",
    "        conversion_ratio = EXCLUDED.conversion_ratio,\n",
    "        conversion_rate = EXCLUDED.conversion_rate,\n",
    "        last_update = EXCLUDED.last_update,\n",
    "        \"Period\" = EXCLUDED.\"Period\",\n",
    "        year = EXCLUDED.year\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        for row in df.itertuples(index=False):\n",
    "            cursor.execute(insert_query, (\n",
    "                row.year_month,\n",
    "                row.group,\n",
    "                row.net_total,\n",
    "                row.opportunity_amount,\n",
    "                row.conversion_ratio,\n",
    "                row.conversion_rate,\n",
    "                row.last_update,\n",
    "                row.Period,\n",
    "                row.year\n",
    "            ))\n",
    "        connection.commit()\n",
    "        logging.info(\"Data insertion/update successful.\")\n",
    "    except psycopg2.Error as e:\n",
    "        logging.error(f\"Error inserting/updating data into PostgreSQL: {e}\")\n",
    "        connection.rollback()  # Rollback changes in case of error\n",
    "\n",
    "# Get current year and month\n",
    "current_datetime = datetime.now()\n",
    "current_year_month = current_datetime.replace(day=1)  # Set day to 1 for 'YYYY-MM-01'\n",
    "\n",
    "# Sample DataFrame creation for debugging purposes\n",
    "data = {\n",
    "    'year_month': ['2024-07-29 17:06:39'] * 5,\n",
    "    'group': ['Assurance', 'Export', 'Gastro + Simplus', 'Machine', 'Spares'],\n",
    "    'net_total': [15568852.67, 7336100.88, 279468.00, 39227007.27, 22421695.40],\n",
    "    'opportunity_amount': [1.396895e+08, 8.532569e+08, 8.198456e+07, 8.293228e+08, 1.475211e+08],\n",
    "    'conversion_ratio': [0.111453, 0.008598, 0.003409, 0.047300, 0.151990],\n",
    "    'conversion_rate': [11.15, 0.86, 0.34, 4.73, 15.20]\n",
    "}\n",
    "merged_df = pd.DataFrame(data)\n",
    "\n",
    "# Print columns for debugging\n",
    "print(\"Columns in merged_df:\", merged_df.columns)\n",
    "\n",
    "try:\n",
    "    connection = psycopg2.connect(**db_config)\n",
    "    cursor = connection.cursor()\n",
    "\n",
    "    # Define table name\n",
    "    table_name = 'SalesMetrics'\n",
    "\n",
    "    # Drop table if it exists\n",
    "    drop_table(cursor, table_name)\n",
    "\n",
    "    # Create table\n",
    "    create_table(cursor)\n",
    "\n",
    "    # Check if current month data exists in the table\n",
    "    if current_month_data_exists(cursor, current_year_month):\n",
    "        # Delete the data for the current month\n",
    "        delete_current_month_data(cursor, current_year_month)\n",
    "\n",
    "    # Insert the new data\n",
    "    insert_or_update_data(merged_df, cursor)\n",
    "\n",
    "except psycopg2.Error as e:\n",
    "    logging.error(f\"Error connecting to PostgreSQL: {e}\")\n",
    "\n",
    "finally:\n",
    "    # Close cursor and connection\n",
    "    if 'connection' in locals() and connection is not None:\n",
    "        cursor.close()\n",
    "        connection.close()\n",
    "        logging.info(\"PostgreSQL connection closed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c856d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed58fed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
